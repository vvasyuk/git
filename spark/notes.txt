val dirs = List("/123/123/123")
spark.read.csv(dirs: _*).filter(col("col1").like("ERROR") && 'col2 === 2).agg(sum(col("col3"))).show.false

FROM CONSOLE
spark-shell --master local
val df = spark.read.csv('/file.csv')

SELECTING DATA
postsDf.select("id", "body")
postsDf.select(postsDf.col("id"), postsDf.col("body"))
postsDf.select(Symbol("id"), Symbol("body"))
postsDf.select('id, 'body)
postsDf.select($"id", $"body")
postsIdBody.drop("body")
df.select(length('col1))

FILTERING DATA
postsIdBody.filter('body contains "Italiano").count
postsDf.filter(('postTypeId === 1) and ('acceptedAnswerId isNull))
postsDf.filter('postTypeId === 1).limit(10)
postsDf.filter(col("col1").like("ERROR") && 'col2 === 2).limit(10)
df.filter(col("a").isin("1", "2"))
df.filter(df("col1") === "value")

SORTING
df.orderBy(desc("col1"))

ADDING AND RENAMING COLUMNS
firstTenQs.withColumnRenamed("ownerUserId", "owner")
postsDf.filter('postTypeId === 1).withColumn("ratio", 'viewCount / 'score).where('ratio < 35).show()
df.withColumn("aa", substring(col(a), 1, 8))
df.withColumn("aa", when(col(a)==="",0).otherwise(1))

USING BUILT-IN SCALAR AND AGGREGATE FUNCTIONS
import org.apache.spark.sql.functions._
postsDf.select(avg('score), max('score), count('score)).show

WINDOW FUNCTIONS
import org.apache.spark.sql.expressions.Window
postsDf.filter('postTypeId === 1).
    select('ownerUserId, 'acceptedAnswerId, 'score, max('score).
        over(Window.partitionBy('ownerUserId)) as "maxPerUser").
        withColumn("toMax", 'maxPerUser - 'score).show(10)
postsDf.filter('postTypeId === 1).
    select('ownerUserId, 'id, 'creationDate,
    lag('id, 1).over(
    Window.partitionBy('ownerUserId).orderBy('creationDate)) as "prev",
    lead('id, 1).over(
    Window.partitionBy('ownerUserId).orderBy('creationDate)) as "next").
    orderBy('ownerUserId, 'id).show(10)

USER-DEFINED FUNCTIONS
val countTags = udf((tags: String) => "&lt;".r.findAllMatchIn(tags).length)
val countTags = spark.udf.register("countTags", (tags: String) => "&lt;".r.findAllMatchIn(tags).length)
postsDf.filter('postTypeId === 1).select('tags, countTags('tags) as "tagCnt").show(10, false)

USER-DEFINED FUNCTIONS WITH CURRYING
import org.apache.spark.sql.functions._
def uDF(strList: List[String]) = udf[String, Int, String, String]((value1: Int, value2: String, value3: String) => value1.toString + "_" + value2 + "_" + value3 + "_" + strList.mkString("_"))
val df = spark.sparkContext.parallelize(Seq((1,"r1c1","r1c2"),(2,"r2c1","r2c2"))).toDF("id","str1","str2")
scala> df.show
+---+----+----+
| id|str1|str2|
+---+----+----+
|  1|r1c1|r1c2|
|  2|r2c1|r2c2|
+---+----+----+
val dummyList = List("dummy1","dummy2")
val result = df.withColumn("new_col", uDF(dummyList)(df("id"),df("str1"),df("str2")))
   scala> result.show(2, false)
+---+----+----+-------------------------+
|id |str1|str2|new_col                  |
+---+----+----+-------------------------+
|1  |r1c1|r1c2|1_r1c1_r1c2_dummy1_dummy2|
|2  |r2c1|r2c2|2_r2c1_r2c2_dummy1_dummy2|
+---+----+----+-------------------------+

# add column
    .withColumn("newCol", lit("123"))
    .withColumn("newCol", col("salary")*100)
    
# regex
    .withColumn("newCol", regex_replace(col("newCol"), "[:,.]", ""))
    
# read csv
    spark.read.option("header", "true").csv("path")
    //.option("ignoreLeadingWhiteSpace", "true")
    //.option("ignoreTraiolingWhiteSpace", "true")
    //.schema(caseClass.getSchema).csv(files: _*).as[caseClass].where("col is not null")
    
# write csv
    df.write.csv("path.csv")
    
# write orc
    import org.apache.spark.sql.SaveMode
    df.write
    //.partitionBy("newCol")
    .mode(SaveMode.Append)
    .orc("path")

# join
    df1.join(df2, df("col1") === df2("col1"), "left")
    
# aggr
    agg(sum(col("col1") as "sum"))
    
# like
    .filter(col("col1").like("pattern"))
    
# count
    df.groupBy("grpCol").agg(sum("col1"), count("id"))
    df.agg(count(lit(1)).alias("cnt"))
    
# filter
    .filter(col("col1") === ("pattern"))
    
# compare
    extends Matchers
    val nums = df.select("a", "b").collect
    val correctNums = List ( Row("1", "2"), Row("1", "2"))
    nums shouldBe(correctNums)
    
    